{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aebd985-c417-4181-b359-ac8f8410e638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[LightGBM] [Info] Number of positive: 458, number of negative: 470\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8829\n",
      "[LightGBM] [Info] Number of data points in the train set: 928, number of used features: 114\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493534 -> initscore=-0.025864\n",
      "[LightGBM] [Info] Start training from score -0.025864\n",
      "Ensemble Model Accuracy: 0.6982758620689655\n",
      "Ensemble Model Precision: 0.6982758620689655\n",
      "Training time: 126.12091398239136 seconds\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 357\u001b[0m\n\u001b[0;32m    354\u001b[0m group_stage_df[initial_features] \u001b[38;5;241m=\u001b[39m imputer\u001b[38;5;241m.\u001b[39mfit_transform(group_stage_df[initial_features])\n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# Adding interaction features to the group stage dataframe\u001b[39;00m\n\u001b[1;32m--> 357\u001b[0m poly_features_group_stage \u001b[38;5;241m=\u001b[39m \u001b[43mpoly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_stage_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43minitial_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m poly_df_group_stage \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(poly_features_group_stage, columns\u001b[38;5;241m=\u001b[39mpoly\u001b[38;5;241m.\u001b[39mget_feature_names_out(initial_features))\n\u001b[0;32m    359\u001b[0m group_stage_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([group_stage_df, poly_df_group_stage], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_polynomial.py:433\u001b[0m, in \u001b[0;36mPolynomialFeatures.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform data to polynomial features.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m    `csr_matrix`.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 433\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    438\u001b[0m max_int32 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to predict and display match results\n",
    "def predict_match_results(df, clf, features):\n",
    "    df_features = df[features]\n",
    "    df_scaled = scaler.transform(df_features)\n",
    "    predictions = clf.predict_proba(df_scaled)[:, 1]\n",
    "    return predictions\n",
    "\n",
    "# Function to display predictions\n",
    "def display_predictions(df):\n",
    "    for index, row in df.iterrows():\n",
    "        home_team = row['home_team']\n",
    "        away_team = row['away_team']\n",
    "        prediction = row['predictions']\n",
    "        print(f\"{away_team} has a probability of {round(prediction, 2)} of beating {home_team}\")\n",
    "\n",
    "# Create a dictionary to map team names to codes\n",
    "team_code_map = {\n",
    "    'Albania': 0, 'Andorra': 1, 'Argentina': 2, 'Armenia': 3, 'Australia': 4, 'Austria': 5, 'Azerbaijan': 6,\n",
    "    'Belarus': 7, 'Belgium': 8, 'Bosnia & Herzegovina': 9, 'Brazil': 10, 'Bulgaria': 11, 'Cameroon': 12,\n",
    "    'Canada': 13, 'Costa Rica': 14, 'Croatia': 15, 'Cyprus': 16, 'Czechia': 17, 'Denmark': 18, 'Ecuador': 19,\n",
    "    'England': 20, 'Estonia': 21, 'Faroe Islands': 22, 'Finland': 23, 'France': 24, 'Georgia': 25, 'Germany': 26,\n",
    "    'Ghana': 27, 'Gibraltar': 28, 'Greece': 29, 'Hungary': 30, 'IR Iran': 31, 'Iceland': 32, 'Israel': 33,\n",
    "    'Italy': 34, 'Japan': 35, 'Kazakhstan': 36, 'Korea Republic': 37, 'Kosovo': 38, 'Latvia': 39, 'Liechtenstein': 40,\n",
    "    'Lithuania': 41, 'Luxembourg': 42, 'Malta': 43, 'Mexico': 44, 'Moldova': 45, 'Montenegro': 46, 'Morocco': 47,\n",
    "    'N. Macedonia': 48, 'Netherlands': 49, 'Northern Ireland': 50, 'Norway': 51, 'Poland': 52, 'Portugal': 53,\n",
    "    'Qatar': 54, 'Rep. of Ireland': 55, 'Romania': 56, 'Russia': 57, 'San Marino': 58, 'Saudi Arabia': 59,\n",
    "    'Scotland': 60, 'Senegal': 61, 'Serbia': 62, 'Slovakia': 63, 'Slovenia': 64, 'Spain': 65, 'Sweden': 66,\n",
    "    'Switzerland': 67, 'Tunisia': 68, 'Türkiye': 69, 'Ukraine': 70, 'United States': 71, 'Uruguay': 72, 'Wales': 73\n",
    "}\n",
    "\n",
    "# Loading datasets\n",
    "nations_one = pd.read_csv(\"nations_league_1.csv\", index_col=0)\n",
    "nations_two = pd.read_csv(\"nations_league_2.csv\", index_col=0)\n",
    "world_cup = pd.read_csv(\"world_cup.csv\", index_col=0)\n",
    "euro_qual = pd.read_csv(\"euro_qual.csv\", index_col=0)\n",
    "euro_2022 = pd.read_csv(\"euro_2022.csv\", index_col=0)\n",
    "\n",
    "# Function to replace abbreviations with full country names\n",
    "def remove_abbreviation(opponent):\n",
    "    return opponent.split(' ', 1)[1]\n",
    "\n",
    "# Combining all df into one combined df, cleaning up data \n",
    "combined = pd.concat([nations_one, nations_two, world_cup, euro_qual, euro_2022])\n",
    "combined['Opponent'] = combined['Opponent'].apply(remove_abbreviation)\n",
    "combined = combined[combined['Comp'] != 'Friendlies (M)']\n",
    "\n",
    "# Function to adjust rows where the match went to overtime and winner was determined by penalty shoot-out\n",
    "def adjust_result(row):\n",
    "    gf = str(row['GF'])\n",
    "    ga = str(row['GA'])\n",
    "    \n",
    "    if re.search(r'\\(\\d+\\)', gf) and re.search(r'\\(\\d+\\)', ga):\n",
    "        gf_shootout = int(re.search(r'\\((\\d+)\\)', gf).group(1))\n",
    "        ga_shootout = int(re.search(r'\\((\\d+)\\)', ga).group(1))\n",
    "        \n",
    "        if gf_shootout > ga_shootout:\n",
    "            return 'W'\n",
    "        elif gf_shootout < ga_shootout:\n",
    "            return 'L'\n",
    "        else:\n",
    "            return row['Result']\n",
    "    else:\n",
    "        return row['Result']\n",
    "\n",
    "combined['GF'] = combined['GF'].astype(str)\n",
    "combined['GA'] = combined['GA'].astype(str)\n",
    "combined['Result'] = combined.apply(adjust_result, axis=1)\n",
    "\n",
    "# Function to create weighted average for goals for and goals against for matches where winner was determined by penalty shootout\n",
    "def adjust_goals(goals):\n",
    "    goals = str(goals)\n",
    "    if re.search(r'\\(\\d+\\)', goals):\n",
    "        regular_goals = int(re.search(r'^\\d+', goals).group())\n",
    "        shootout_goals = int(re.search(r'\\((\\d+)\\)', goals).group(1))\n",
    "        adjusted_goals = (regular_goals + shootout_goals) / 2\n",
    "        return adjusted_goals\n",
    "    else:\n",
    "        return float(goals)\n",
    "\n",
    "combined['GF'] = combined['GF'].apply(adjust_goals)\n",
    "combined['GA'] = combined['GA'].apply(adjust_goals)\n",
    "\n",
    "combined.columns = combined.columns.str.lower()\n",
    "combined = combined.sort_values(by=\"date\")\n",
    "\n",
    "venue_mapping = {'Home': 1, 'Away': 2, 'Neutral': 3}\n",
    "combined['venue_num'] = combined['venue'].map(venue_mapping).astype(int)\n",
    "\n",
    "# Convert target values to binary (0 for loss, 1 for win)\n",
    "result_mapping = {'L': 0, 'D': 0, 'W': 1}\n",
    "combined = combined.dropna(subset=['result'])\n",
    "combined['target'] = combined['result'].map(result_mapping).astype(int)\n",
    "combined = combined.dropna(subset=['saves'])\n",
    "combined['saves'] = combined['saves'].astype(int)\n",
    "combined = combined.drop(columns=['xg', 'xga'])\n",
    "\n",
    "# Function to create rolling avg for stats\n",
    "def rolling_avg(group, cols, new_cols):\n",
    "    group = group.sort_values(\"date\")\n",
    "    rolling_stats = group[cols].rolling(3, closed='left').mean()\n",
    "    group[new_cols] = rolling_stats\n",
    "    group = group.dropna(subset=new_cols)\n",
    "    return group\n",
    "\n",
    "cols = [\"gf\", \"ga\", \"sh\", \"sot\", \"pk\", \"pkatt\", \"saves\", \"cs\"]\n",
    "new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "\n",
    "combined_rolling = combined.groupby('nation').apply(lambda x: rolling_avg(x, cols, new_cols))\n",
    "combined_rolling = combined_rolling.droplevel('nation')\n",
    "combined_rolling = combined_rolling.sort_values(by=\"date\")\n",
    "\n",
    "# Adding additional feature columns\n",
    "combined_rolling[\"venue_code\"] = combined_rolling[\"venue\"].astype(\"category\").cat.codes\n",
    "combined_rolling[\"opp_code\"] = combined_rolling[\"opponent\"].astype(\"category\").cat.codes\n",
    "combined_rolling[\"hour\"] = combined_rolling[\"time\"].str.replace(\":.+\", \"\", regex=True).astype(int)\n",
    "combined_rolling[\"date\"] = pd.to_datetime(combined_rolling[\"date\"])\n",
    "combined_rolling[\"day_code\"] = combined_rolling[\"date\"].dt.dayofweek\n",
    "\n",
    "# Define initial features\n",
    "initial_features = ['gf_rolling', 'ga_rolling', 'sh_rolling', 'sot_rolling', 'pk_rolling', 'pkatt_rolling', 'saves_rolling', 'cs_rolling',\n",
    "                    'venue_code', 'opp_code', 'hour', 'day_code']\n",
    "\n",
    "# Create interaction features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "poly_features = poly.fit_transform(combined_rolling[initial_features])\n",
    "poly_feature_names = poly.get_feature_names_out(initial_features)\n",
    "poly_df = pd.DataFrame(poly_features, columns=poly_feature_names, index=combined_rolling.index)\n",
    "combined_rolling = pd.concat([combined_rolling, poly_df], axis=1)\n",
    "\n",
    "# Define the final features to use for each team\n",
    "features = initial_features + list(poly_feature_names)\n",
    "\n",
    "# Splitting the dataset into train and test set (80-20 split for better generalization)\n",
    "X = combined_rolling[features]\n",
    "y = combined_rolling['target']\n",
    "\n",
    "# Handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter tuning for RandomForest using RandomizedSearchCV\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf_random_search = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                                      param_distributions=rf_param_grid,\n",
    "                                      scoring='accuracy',\n",
    "                                      n_iter=10,\n",
    "                                      n_jobs=-1,\n",
    "                                      cv=3,\n",
    "                                      verbose=2,\n",
    "                                      random_state=42)\n",
    "\n",
    "rf_random_search.fit(X_train, y_train)\n",
    "best_rf_params = rf_random_search.best_params_\n",
    "\n",
    "# Hyperparameter tuning for Logistic Regression using RandomizedSearchCV\n",
    "lr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "lr_random_search = RandomizedSearchCV(estimator=LogisticRegression(random_state=42),\n",
    "                                      param_distributions=lr_param_grid,\n",
    "                                      scoring='accuracy',\n",
    "                                      n_iter=10,\n",
    "                                      n_jobs=-1,\n",
    "                                      cv=3,\n",
    "                                      verbose=2,\n",
    "                                      random_state=42)\n",
    "\n",
    "lr_random_search.fit(X_train, y_train)\n",
    "best_lr_params = lr_random_search.best_params_\n",
    "\n",
    "# Hyperparameter tuning for XGBoost using RandomizedSearchCV\n",
    "xgb_param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'colsample_bytree': [0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "xgb_random_search = RandomizedSearchCV(estimator=xgb.XGBClassifier(objective='binary:hinge'),\n",
    "                                       param_distributions=xgb_param_grid,\n",
    "                                       scoring='accuracy',\n",
    "                                       n_iter=10,\n",
    "                                       n_jobs=-1,\n",
    "                                       cv=3,\n",
    "                                       verbose=2,\n",
    "                                       random_state=42)\n",
    "\n",
    "xgb_random_search.fit(X_train, y_train)\n",
    "best_xgb_params = xgb_random_search.best_params_\n",
    "\n",
    "# Hyperparameter tuning for CatBoost using RandomizedSearchCV\n",
    "cb_param_grid = {\n",
    "    'depth': [4, 6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "    'iterations': [100, 200, 300, 500]\n",
    "}\n",
    "\n",
    "cb_random_search = RandomizedSearchCV(estimator=cb.CatBoostClassifier(verbose=0, random_state=42),\n",
    "                                      param_distributions=cb_param_grid,\n",
    "                                      scoring='accuracy',\n",
    "                                      n_iter=10,\n",
    "                                      n_jobs=-1,\n",
    "                                      cv=3,\n",
    "                                      verbose=2,\n",
    "                                      random_state=42)\n",
    "\n",
    "cb_random_search.fit(X_train, y_train)\n",
    "best_cb_params = cb_random_search.best_params_\n",
    "\n",
    "# Train models with best hyperparameters\n",
    "log_clf = LogisticRegression(**best_lr_params, random_state=42)\n",
    "rf_clf = RandomForestClassifier(**best_rf_params, random_state=42)\n",
    "xgb_clf = xgb.XGBClassifier(**best_xgb_params, objective='binary:hinge')\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "lgb_clf = lgb.LGBMClassifier(random_state=42)\n",
    "cb_clf = cb.CatBoostClassifier(**best_cb_params, verbose=0, random_state=42)\n",
    "\n",
    "# Ensemble with VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('lr', log_clf),\n",
    "    ('rf', rf_clf),\n",
    "    ('xgb', xgb_clf),\n",
    "    ('gb', gb_clf),\n",
    "    ('lgb', lgb_clf),\n",
    "    ('cb', cb_clf)\n",
    "], voting='soft')  # Use 'soft' voting\n",
    "\n",
    "start_time = time.time()\n",
    "voting_clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and precision for ensemble\n",
    "accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
    "precision_voting = precision_score(y_test, y_pred_voting, average='weighted')\n",
    "\n",
    "print(f'Ensemble Model Accuracy: {accuracy_voting}')\n",
    "print(f'Ensemble Model Precision: {precision_voting}')\n",
    "print(f'Training time: {end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b86311-55a8-4d0a-a46f-16afd92cc7f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 22 features, but PolynomialFeatures is expecting 114 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 129\u001b[0m\n\u001b[0;32m    126\u001b[0m poly\u001b[38;5;241m.\u001b[39mfit(X_train)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Ensure the features are in the same order as used in fitting PolynomialFeatures\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m poly_features_group_stage \u001b[38;5;241m=\u001b[39m \u001b[43mpoly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_stage_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43minitial_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m poly_df_group_stage \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(poly_features_group_stage, columns\u001b[38;5;241m=\u001b[39mpoly\u001b[38;5;241m.\u001b[39mget_feature_names_out(initial_features))\n\u001b[0;32m    131\u001b[0m group_stage_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([group_stage_df, poly_df_group_stage], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_polynomial.py:433\u001b[0m, in \u001b[0;36mPolynomialFeatures.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform data to polynomial features.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m    `csr_matrix`.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 433\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    438\u001b[0m max_int32 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 22 features, but PolynomialFeatures is expecting 114 features as input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to predict and display match results\n",
    "def predict_match_results(df, clf, features):\n",
    "    df_features = df[features]\n",
    "    df_scaled = scaler.transform(df_features)\n",
    "    predictions = clf.predict_proba(df_scaled)[:, 1]\n",
    "    return predictions\n",
    "\n",
    "# Create a dictionary to map team names to codes\n",
    "team_code_map = {\n",
    "    'Albania': 0, 'Andorra': 1, 'Argentina': 2, 'Armenia': 3, 'Australia': 4, 'Austria': 5, 'Azerbaijan': 6,\n",
    "    'Belarus': 7, 'Belgium': 8, 'Bosnia & Herzegovina': 9, 'Brazil': 10, 'Bulgaria': 11, 'Cameroon': 12,\n",
    "    'Canada': 13, 'Costa Rica': 14, 'Croatia': 15, 'Cyprus': 16, 'Czechia': 17, 'Denmark': 18, 'Ecuador': 19,\n",
    "    'England': 20, 'Estonia': 21, 'Faroe Islands': 22, 'Finland': 23, 'France': 24, 'Georgia': 25, 'Germany': 26,\n",
    "    'Ghana': 27, 'Gibraltar': 28, 'Greece': 29, 'Hungary': 30, 'IR Iran': 31, 'Iceland': 32, 'Israel': 33,\n",
    "    'Italy': 34, 'Japan': 35, 'Kazakhstan': 36, 'Korea Republic': 37, 'Kosovo': 38, 'Latvia': 39, 'Liechtenstein': 40,\n",
    "    'Lithuania': 41, 'Luxembourg': 42, 'Malta': 43, 'Mexico': 44, 'Moldova': 45, 'Montenegro': 46, 'Morocco': 47,\n",
    "    'N. Macedonia': 48, 'Netherlands': 49, 'Northern Ireland': 50, 'Norway': 51, 'Poland': 52, 'Portugal': 53,\n",
    "    'Qatar': 54, 'Rep. of Ireland': 55, 'Romania': 56, 'Russia': 57, 'San Marino': 58, 'Saudi Arabia': 59,\n",
    "    'Scotland': 60, 'Senegal': 61, 'Serbia': 62, 'Slovakia': 63, 'Slovenia': 64, 'Spain': 65, 'Sweden': 66,\n",
    "    'Switzerland': 67, 'Tunisia': 68, 'Türkiye': 69, 'Ukraine': 70, 'United States': 71, 'Uruguay': 72, 'Wales': 73\n",
    "}\n",
    "\n",
    "# Creating the DataFrame for the match data\n",
    "group_stage_matches = [\n",
    "    {'home_team': 'Germany', 'away_team': 'Scotland'},\n",
    "    {'home_team': 'Hungary', 'away_team': 'Switzerland'},\n",
    "    {'home_team': 'Spain', 'away_team': 'Croatia'},\n",
    "    {'home_team': 'Italy', 'away_team': 'Albania'},\n",
    "    {'home_team': 'Poland', 'away_team': 'Netherlands'},\n",
    "    {'home_team': 'Slovenia', 'away_team': 'Denmark'},\n",
    "    {'home_team': 'Serbia', 'away_team': 'England'},\n",
    "    {'home_team': 'Romania', 'away_team': 'Ukraine'},\n",
    "    {'home_team': 'Belgium', 'away_team': 'Slovakia'},\n",
    "    {'home_team': 'Austria', 'away_team': 'France'},\n",
    "    {'home_team': 'Turkey', 'away_team': 'Georgia'},\n",
    "    {'home_team': 'Portugal', 'away_team': 'Czechia'},\n",
    "    {'home_team': 'Croatia', 'away_team': 'Albania'},\n",
    "    {'home_team': 'Germany', 'away_team': 'Hungary'},\n",
    "    {'home_team': 'Scotland', 'away_team': 'Switzerland'},\n",
    "    {'home_team': 'Slovenia', 'away_team': 'Serbia'},\n",
    "    {'home_team': 'Denmark', 'away_team': 'England'},\n",
    "    {'home_team': 'Spain', 'away_team': 'Italy'},\n",
    "    {'home_team': 'Slovakia', 'away_team': 'Ukraine'},\n",
    "    {'home_team': 'Poland', 'away_team': 'Austria'},\n",
    "    {'home_team': 'Netherlands', 'away_team': 'France'},\n",
    "    {'home_team': 'Georgia', 'away_team': 'Czechia'},\n",
    "    {'home_team': 'Turkey', 'away_team': 'Portugal'},\n",
    "    {'home_team': 'Belgium', 'away_team': 'Romania'},\n",
    "    {'home_team': 'Switzerland', 'away_team': 'Germany'},\n",
    "    {'home_team': 'Scotland', 'away_team': 'Hungary'},\n",
    "    {'home_team': 'Albania', 'away_team': 'Spain'},\n",
    "    {'home_team': 'Croatia', 'away_team': 'Italy'},\n",
    "    {'home_team': 'France', 'away_team': 'Poland'},\n",
    "    {'home_team': 'Netherlands', 'away_team': 'Austria'},\n",
    "    {'home_team': 'England', 'away_team': 'Slovenia'},\n",
    "    {'home_team': 'Denmark', 'away_team': 'Serbia'},\n",
    "    {'home_team': 'Ukraine', 'away_team': 'Belgium'},\n",
    "    {'home_team': 'Slovakia', 'away_team': 'Romania'},\n",
    "    {'home_team': 'Czechia', 'away_team': 'Turkey'},\n",
    "    {'home_team': 'Georgia', 'away_team': 'Portugal'}\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "group_stage_df = pd.DataFrame(group_stage_matches)\n",
    "\n",
    "# Adding additional columns needed for prediction\n",
    "group_stage_df['venue_code'] = group_stage_df.apply(lambda x: 1 if x['home_team'] == 'Germany' else 3, axis=1)  # Germany as home\n",
    "\n",
    "# Map team names to codes\n",
    "group_stage_df['home_code'] = group_stage_df['home_team'].map(team_code_map)\n",
    "group_stage_df['opp_code'] = group_stage_df['away_team'].map(team_code_map)\n",
    "\n",
    "# Extracting the latest rolling statistics for each home team\n",
    "latest_stats = combined_rolling.groupby('nation').last().reset_index()\n",
    "\n",
    "# Renaming columns in latest_stats to match with features\n",
    "latest_stats = latest_stats.rename(columns={'nation': 'home_team', 'gf_rolling': 'gf_rolling_latest',\n",
    "                                            'ga_rolling': 'ga_rolling_latest', 'sh_rolling': 'sh_rolling_latest',\n",
    "                                            'sot_rolling': 'sot_rolling_latest', 'pk_rolling': 'pk_rolling_latest',\n",
    "                                            'pkatt_rolling': 'pkatt_rolling_latest', 'saves_rolling': 'saves_rolling_latest',\n",
    "                                            'cs_rolling': 'cs_rolling_latest'})\n",
    "\n",
    "# Merging the latest statistics with the group stage dataframe\n",
    "group_stage_df = group_stage_df.merge(latest_stats, how='left', on='home_team')\n",
    "\n",
    "# Renaming the columns to match the initial features\n",
    "group_stage_df.rename(columns={'gf_rolling_latest': 'gf_rolling', 'ga_rolling_latest': 'ga_rolling', 'sh_rolling_latest': 'sh_rolling',\n",
    "                               'sot_rolling_latest': 'sot_rolling', 'pk_rolling_latest': 'pk_rolling', 'pkatt_rolling_latest': 'pkatt_rolling',\n",
    "                               'saves_rolling_latest': 'saves_rolling', 'cs_rolling_latest': 'cs_rolling'}, inplace=True)\n",
    "\n",
    "# Adding necessary columns that might be missing\n",
    "initial_features = ['gf_rolling', 'ga_rolling', 'sh_rolling', 'sot_rolling', 'pk_rolling', 'pkatt_rolling',\n",
    "                    'saves_rolling', 'cs_rolling', 'venue_code', 'opp_code', 'hour', 'day_code']\n",
    "\n",
    "for col in initial_features:\n",
    "    if col not in group_stage_df.columns:\n",
    "        group_stage_df[col] = 0\n",
    "\n",
    "# Ensure the order of initial features\n",
    "group_stage_df = group_stage_df[initial_features + ['home_team', 'away_team']]\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "group_stage_df[initial_features] = imputer.fit_transform(group_stage_df[initial_features])\n",
    "\n",
    "# Fit the PolynomialFeatures instance on the training data\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "\n",
    "# Ensure the features are in the same order as used in fitting PolynomialFeatures\n",
    "poly_features_group_stage = poly.transform(group_stage_df[initial_features])\n",
    "poly_df_group_stage = pd.DataFrame(poly_features_group_stage, columns=poly.get_feature_names_out(initial_features))\n",
    "group_stage_df = pd.concat([group_stage_df, poly_df_group_stage], axis=1)\n",
    "\n",
    "# Predicting group stage outcomes\n",
    "group_stage_df['predictions'] = predict_match_results(group_stage_df, voting_clf, features)\n",
    "\n",
    "# Display group stage predictions\n",
    "def display_predictions(df):\n",
    "    for index, row in df.iterrows():\n",
    "        home_team = row['home_team']\n",
    "        away_team = row['away_team']\n",
    "        prediction = row['predictions']\n",
    "        print(f\"{away_team} has a probability of {round(prediction, 2)} of beating {home_team}\")\n",
    "\n",
    "display_predictions(group_stage_df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
